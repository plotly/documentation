{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing classification one often wants to predict not only the class label, but also the associated probability. This probability gives some kind of confidence on the prediction. This example demonstrates how to display how well calibrated the predicted probabilities are and how to calibrate an uncalibrated classifier.\n",
    "\n",
    "The experiment is performed on an artificial dataset for binary classification with 100.000 samples (1.000 of them are used for model fitting) with 20 features. Of the 20 features, only 2 are informative and 10 are redundant. The first figure shows the estimated probabilities obtained with logistic regression, Gaussian naive Bayes, and Gaussian naive Bayes with both isotonic calibration and sigmoid calibration. The calibration performance is evaluated with Brier score, reported in the legend (the smaller the better). One can observe here that logistic regression is well calibrated while raw Gaussian naive Bayes performs very badly. This is because of the redundant features which violate the assumption of feature-independence and result in an overly confident classifier, which is indicated by the typical transposed-sigmoid curve.\n",
    "\n",
    "Calibration of the probabilities of Gaussian naive Bayes with isotonic regression can fix this issue as can be seen from the nearly diagonal calibration curve. Sigmoid calibration also improves the brier score slightly, albeit not as strongly as the non-parametric isotonic regression. This can be attributed to the fact that we have plenty of calibration data such that the greater flexibility of the non-parametric model can be exploited.\n",
    "\n",
    "The second figure shows the calibration curve of a linear support-vector classifier (LinearSVC). LinearSVC shows the opposite behavior as Gaussian naive Bayes: the calibration curve has a sigmoid curve, which is typical for an under-confident classifier. In the case of LinearSVC, this is caused by the margin property of the hinge loss, which lets the model focus on hard samples that are close to the decision boundary (the support vectors).\n",
    "Both kinds of calibration can fix this issue and yield nearly identical results. This shows that sigmoid calibration can deal with situations where the calibration curve of the base classifier is sigmoid (e.g., for LinearSVC) but not where it is transposed-sigmoid (e.g., Gaussian naive Bayes).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New to Plotly?\n",
    "Plotly's Python library is free and open source! [Get started](https://plot.ly/python/getting-started/) by downloading the client and [reading the primer](https://plot.ly/python/getting-started/).\n",
    "<br>You can set up Plotly to work in [online](https://plot.ly/python/getting-started/#initialization-for-online-plotting) or [offline](https://plot.ly/python/getting-started/#initialization-for-offline-plotting) mode, or in [jupyter notebooks](https://plot.ly/python/getting-started/#start-plotting-online).\n",
    "<br>We also have a quick-reference [cheatsheet](https://images.plot.ly/plotly-documentation/images/python_cheat_sheet.pdf) (new!) to help you get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.18'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "print(__doc__)\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n",
    "                             f1_score)\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculations and Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "X, y = datasets.make_classification(n_samples=100000, n_features=20,\n",
    "                                    n_informative=2, n_redundant=10,\n",
    "                                    random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.99,\n",
    "                                                    random_state=42)\n",
    "\n",
    "\n",
    "def plot_calibration_curve(est, name):\n",
    "    \"\"\"Plot calibration curve for est w/o and with calibration. \"\"\"\n",
    "    # Calibrated with isotonic calibration\n",
    "    isotonic = CalibratedClassifierCV(est, cv=2, method='isotonic')\n",
    "\n",
    "    # Calibrated with sigmoid calibration\n",
    "    sigmoid = CalibratedClassifierCV(est, cv=2, method='sigmoid')\n",
    "\n",
    "    # Logistic regression with no calibration as baseline\n",
    "    lr = LogisticRegression(C=1., solver='lbfgs')\n",
    "\n",
    "    fig = tools.make_subplots(rows=2, cols=1)\n",
    "\n",
    "    Perfectly_calibrated_trace = go.Scatter(x=[0, 1], y=[0, 1], \n",
    "                                            name=\"Perfectly calibrated\",\n",
    "                                            mode='lines',\n",
    "                                            line = dict(color='black', width=1,\n",
    "                                                   dash='dash'),)\n",
    "    i=0\n",
    "    Calibration_Lines_Plot=[]\n",
    "    Calibration_Lines_Plot.append(Perfectly_calibrated_trace)\n",
    "    Calibration_Histograms_Plot=[]\n",
    "    colors=['blue','green','red','cyan']\n",
    "\n",
    "    for clf, name in [(lr, 'Logistic'),\n",
    "                      (est, name),\n",
    "                      (isotonic, name + ' + Isotonic'),\n",
    "                      (sigmoid, name + ' + Sigmoid')]:\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        if hasattr(clf, \"predict_proba\"):\n",
    "            prob_pos = clf.predict_proba(X_test)[:, 1]\n",
    "        else:  # use decision function\n",
    "            prob_pos = clf.decision_function(X_test)\n",
    "            prob_pos = \\\n",
    "                (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())\n",
    "\n",
    "        clf_score = brier_score_loss(y_test, prob_pos, pos_label=y.max())\n",
    "        print(\"%s:\" % name)\n",
    "        print(\"\\tBrier: %1.3f\" % (clf_score))\n",
    "        print(\"\\tPrecision: %1.3f\" % precision_score(y_test, y_pred))\n",
    "        print(\"\\tRecall: %1.3f\" % recall_score(y_test, y_pred))\n",
    "        print(\"\\tF1: %1.3f\\n\" % f1_score(y_test, y_pred))\n",
    "\n",
    "        fraction_of_positives, mean_predicted_value = \\\n",
    "            calibration_curve(y_test, prob_pos, n_bins=10)\n",
    "\n",
    "        trace1 = go.Scatter(x=mean_predicted_value, y=fraction_of_positives,\n",
    "                            line=dict(color=colors[i], width=1,),\n",
    "                            name=\"%s\" % (name))\n",
    "        Calibration_Lines_Plot.append(trace1)\n",
    "\n",
    "        trace2 = go.Histogram(x=prob_pos,  name=name, nbinsx=10,\n",
    "                               marker=dict(color=colors[i]),\n",
    "                               opacity=0.75, showlegend=False\n",
    "                             )\n",
    "        Calibration_Histograms_Plot.append(trace2)\n",
    "        i=i+1\n",
    "\n",
    "    for i in range(len( Calibration_Lines_Plot)):\n",
    "        fig.append_trace( Calibration_Lines_Plot[i], 1, 1)\n",
    "\n",
    "    for i in range(len( Calibration_Histograms_Plot)):\n",
    "        fig.append_trace( Calibration_Histograms_Plot[i], 2, 1)\n",
    "\n",
    "    fig['layout']['yaxis1'].update(title='Fraction of positives',\n",
    "                                  range=[-0.05, 1.05])\n",
    "    fig['layout']['yaxis2'].update(title='Count')\n",
    "    fig['layout']['xaxis2'].update(title='Mean predicted value')\n",
    "\n",
    "    fig['layout'].update(title='Calibration plots  (reliability curve)',\n",
    "                         barmode='overlay', height=1000)\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the format of your plot grid:\n",
      "[ (1,1) x1,y1 ]\n",
      "[ (2,1) x2,y2 ]\n",
      "\n",
      "Logistic:\n",
      "\tBrier: 0.099\n",
      "\tPrecision: 0.872\n",
      "\tRecall: 0.851\n",
      "\tF1: 0.862\n",
      "\n",
      "Naive Bayes:\n",
      "\tBrier: 0.118\n",
      "\tPrecision: 0.857\n",
      "\tRecall: 0.876\n",
      "\tF1: 0.867\n",
      "\n",
      "Naive Bayes + Isotonic:\n",
      "\tBrier: 0.098\n",
      "\tPrecision: 0.883\n",
      "\tRecall: 0.836\n",
      "\tF1: 0.859\n",
      "\n",
      "Naive Bayes + Sigmoid:\n",
      "\tBrier: 0.109\n",
      "\tPrecision: 0.861\n",
      "\tRecall: 0.871\n",
      "\tF1: 0.866\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diksha/.local/lib/python2.7/site-packages/plotly/plotly/plotly.py:236: UserWarning:\n",
      "\n",
      "Woah there! Look at all those points! Due to browser limitations, the Plotly SVG drawing functions have a hard time graphing more than 500k data points for line charts, or 40k points for other types of charts. Here are some suggestions:\n",
      "(1) Use the `plotly.graph_objs.Scattergl` trace object to generate a WebGl graph.\n",
      "(2) Trying using the image API to return an image instead of a graph URL\n",
      "(3) Use matplotlib\n",
      "(4) See if you can create your visualization with fewer data points\n",
      "\n",
      "If the visualization you're using aggregates points (e.g., box plot, histogram, etc.) you can disregard this warning.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Diksha_Gabha/2704.embed\" height=\"1000px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot calibration curve for Gaussian Naive Baye\n",
    "py.iplot(plot_calibration_curve(GaussianNB(), \"Naive Bayes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the format of your plot grid:\n",
      "[ (1,1) x1,y1 ]\n",
      "[ (2,1) x2,y2 ]\n",
      "\n",
      "Logistic:\n",
      "\tBrier: 0.099\n",
      "\tPrecision: 0.872\n",
      "\tRecall: 0.851\n",
      "\tF1: 0.862\n",
      "\n",
      "SVC:\n",
      "\tBrier: 0.163\n",
      "\tPrecision: 0.872\n",
      "\tRecall: 0.852\n",
      "\tF1: 0.862\n",
      "\n",
      "SVC + Isotonic:\n",
      "\tBrier: 0.100\n",
      "\tPrecision: 0.853\n",
      "\tRecall: 0.878\n",
      "\tF1: 0.865\n",
      "\n",
      "SVC + Sigmoid:\n",
      "\tBrier: 0.099\n",
      "\tPrecision: 0.874\n",
      "\tRecall: 0.849\n",
      "\tF1: 0.861\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Diksha_Gabha/2706.embed\" height=\"1000px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot calibration curve for Linear SVC\n",
    "py.iplot(plot_calibration_curve(LinearSVC(), \"SVC\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author:  \n",
    "         \n",
    "         Alexandre Gramfort <alexandre.gramfort@telecom-paristech.fr>\n",
    "         Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n",
    "\n",
    "License: \n",
    "        \n",
    "         BSD Style.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href=\"//fonts.googleapis.com/css?family=Open+Sans:600,400,300,200|Inconsolata|Ubuntu+Mono:400,700\" rel=\"stylesheet\" type=\"text/css\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"http://help.plot.ly/documentation/all_static/css/ipython-notebook-custom.css\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/plotly/publisher.git\n",
      "  Cloning https://github.com/plotly/publisher.git to /tmp/pip-niqONb-build\n",
      "Installing collected packages: publisher\n",
      "  Running setup.py install for publisher ... \u001b[?25l-\b \berror\n",
      "    Complete output from command /usr/bin/python -u -c \"import setuptools, tokenize;__file__='/tmp/pip-niqONb-build/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))\" install --record /tmp/pip-w32S2q-record/install-record.txt --single-version-externally-managed --compile:\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build/lib.linux-x86_64-2.7\n",
      "    creating build/lib.linux-x86_64-2.7/publisher\n",
      "    copying publisher/publisher.py -> build/lib.linux-x86_64-2.7/publisher\n",
      "    copying publisher/__init__.py -> build/lib.linux-x86_64-2.7/publisher\n",
      "    running install_lib\n",
      "    creating /usr/local/lib/python2.7/dist-packages/publisher\n",
      "    error: could not create '/usr/local/lib/python2.7/dist-packages/publisher': Permission denied\n",
      "    \n",
      "    ----------------------------------------\n",
      "\u001b[31mCommand \"/usr/bin/python -u -c \"import setuptools, tokenize;__file__='/tmp/pip-niqONb-build/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))\" install --record /tmp/pip-w32S2q-record/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in /tmp/pip-niqONb-build/\u001b[0m\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML('<link href=\"//fonts.googleapis.com/css?family=Open+Sans:600,400,300,200|Inconsolata|Ubuntu+Mono:400,700\" rel=\"stylesheet\" type=\"text/css\" />'))\n",
    "display(HTML('<link rel=\"stylesheet\" type=\"text/css\" href=\"http://help.plot.ly/documentation/all_static/css/ipython-notebook-custom.css\">'))\n",
    "\n",
    "! pip install git+https://github.com/plotly/publisher.git --upgrade\n",
    "import publisher\n",
    "publisher.publish(\n",
    "    'probability-calibration-curve.ipynb', 'scikit-learn/plot-calibration-curve/', 'Probability Calibration Curves | plotly',\n",
    "    ' ',\n",
    "    title = 'Probability Calibration Curves | plotly',\n",
    "    name = 'Probability Calibration Curves',\n",
    "    has_thumbnail='true', thumbnail='thumbnail/calibration-3.jpg', \n",
    "    language='scikit-learn', page_type='example_index',\n",
    "    display_as='calibration', order=3,\n",
    "    ipynb= '~Diksha_Gabha/2708')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
