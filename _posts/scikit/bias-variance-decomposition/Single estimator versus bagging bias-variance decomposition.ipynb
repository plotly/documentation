{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example illustrates and compares the bias-variance decomposition of the expected mean squared error of a single estimator against a bagging ensemble.\n",
    "\n",
    "In regression, the expected mean squared error of an estimator can be decomposed in terms of bias, variance and noise. On average over datasets of the regression problem, the bias term measures the average amount by which the predictions of the estimator differ from the predictions of the best possible estimator for the problem (i.e., the Bayes model). The variance term measures the variability of the predictions of the estimator when fit over different instances LS of the problem. Finally, the noise measures the irreducible part of the error which is due the variability in the data.\n",
    "\n",
    "The upper left figure illustrates the predictions (in dark red) of a single decision tree trained over a random dataset LS (the blue dots) of a toy 1d regression problem. It also illustrates the predictions (in light red) of other single decision trees trained over other (and different) randomly drawn instances LS of the problem. \n",
    "\n",
    "Intuitively, the variance term here corresponds to the width of the beam of predictions (in light red) of the individual estimators. The larger the variance, the more sensitive are the predictions for x to small changes in the training set. The bias term corresponds to the difference between the average prediction of the estimator (in cyan) and the best possible model (in dark blue). On this problem, we can thus observe that the bias is quite low (both the cyan and the blue curves are close to each other) while the variance is large (the red beam is rather wide).\n",
    "\n",
    "The lower left figure plots the pointwise decomposition of the expected mean squared error of a single decision tree. It confirms that the bias term (in blue) is low while the variance is large (in green). It also illustrates the noise part of the error which, as expected, appears to be constant and around 0.01.\n",
    "\n",
    "The right figures correspond to the same plots but using instead a bagging ensemble of decision trees. In both figures, we can observe that the bias term is larger than in the previous case. In the upper right figure, the difference between the average prediction (in cyan) and the best possible model is larger (e.g., notice the offset around x=2). In the lower right figure, the bias curve is also slightly higher than in the lower left figure. In terms of variance however, the beam of predictions is narrower, which suggests that the variance is lower. Indeed, as the lower right figure confirms, the variance term (in green) is lower than for single decision trees. Overall, the bias- variance decomposition is therefore no longer the same. The tradeoff is better for bagging: averaging several decision trees fit on bootstrap copies of the dataset slightly increases the bias term but allows for a larger reduction of the variance, which results in a lower overall mean squared error (compare the red curves int the lower figures). The script output also confirms this intuition. The total error of the bagging ensemble is lower than the total error of a single decision tree, and this difference indeed mainly stems from a reduced variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New to Plotly?\n",
    "Plotly's Python library is free and open source! [Get started](https://plot.ly/python/getting-started/) by downloading the client and [reading the primer](https://plot.ly/python/getting-started/).\n",
    "<br>You can set up Plotly to work in [online](https://plot.ly/python/getting-started/#initialization-for-online-plotting) or [offline](https://plot.ly/python/getting-started/#initialization-for-offline-plotting) mode, or in [jupyter notebooks](https://plot.ly/python/getting-started/#start-plotting-online).\n",
    "<br>We also have a quick-reference [cheatsheet](https://images.plot.ly/plotly-documentation/images/python_cheat_sheet.pdf) (new!) to help you get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.18.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial imports [BaggingRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html#sklearn.ensemble.BaggingRegressor) and [DecisionTreeRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_repeat = 50       # Number of iterations for computing expectations\n",
    "n_train = 50        # Size of the training set\n",
    "n_test = 1000       # Size of the test set\n",
    "noise = 0.1         # Standard deviation of the noise\n",
    "np.random.seed(0)\n",
    "\n",
    "# Change this for exploring the bias-variance decomposition of other\n",
    "# estimators. This should work well for estimators with high variance (e.g.,\n",
    "# decision trees or KNN), but poorly for estimators with low variance (e.g.,\n",
    "# linear models).\n",
    "estimators = [(\"Tree\", DecisionTreeRegressor()),\n",
    "              (\"Bagging(Tree)\", BaggingRegressor(DecisionTreeRegressor()))]\n",
    "\n",
    "n_estimators = len(estimators)\n",
    "\n",
    "# Generate data\n",
    "def f(x):\n",
    "    x = x.ravel()\n",
    "\n",
    "    return np.exp(-x ** 2) + 1.5 * np.exp(-(x - 2) ** 2)\n",
    "\n",
    "def generate(n_samples, noise, n_repeat=1):\n",
    "    X = np.random.rand(n_samples) * 10 - 5\n",
    "    X = np.sort(X)\n",
    "\n",
    "    if n_repeat == 1:\n",
    "        y = f(X) + np.random.normal(0.0, noise, n_samples)\n",
    "    else:\n",
    "        y = np.zeros((n_samples, n_repeat))\n",
    "\n",
    "        for i in range(n_repeat):\n",
    "            y[:, i] = f(X) + np.random.normal(0.0, noise, n_samples)\n",
    "\n",
    "    X = X.reshape((n_samples, 1))\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(n_repeat):\n",
    "    X, y = generate(n_samples=n_train, noise=noise)\n",
    "    X_train.append(X)\n",
    "    y_train.append(y)\n",
    "\n",
    "X_test, y_test = generate(n_samples=n_test, noise=noise, n_repeat=n_repeat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_to_plotly(x):\n",
    "    k = []\n",
    "    \n",
    "    for i in range(0, len(x)):\n",
    "        k.append(x[i][0])\n",
    "        \n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the format of your plot grid:\n",
      "[ (1,1) x1,y1 ]  [ (1,2) x2,y2 ]\n",
      "[ (2,1) x3,y3 ]  [ (2,2) x4,y4 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fig = tools.make_subplots(rows=2, cols=2, \n",
    "                          subplot_titles=('Tree', 'Bagging(tree)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree: 0.0255 (error) = 0.0003 (bias^2)  + 0.0152 (var) + 0.0098 (noise)\n",
      "Bagging(Tree): 0.0196 (error) = 0.0004 (bias^2)  + 0.0092 (var) + 0.0098 (noise)\n"
     ]
    }
   ],
   "source": [
    "col = 1\n",
    "row = 1\n",
    "\n",
    "for n, (name, estimator) in enumerate(estimators):\n",
    "    # Compute predictions\n",
    "    y_predict = np.zeros((n_test, n_repeat))\n",
    "\n",
    "    for i in range(n_repeat):\n",
    "        estimator.fit(X_train[i], y_train[i])\n",
    "        y_predict[:, i] = estimator.predict(X_test)\n",
    "\n",
    "    # Bias^2 + Variance + Noise decomposition of the mean squared error\n",
    "    y_error = np.zeros(n_test)\n",
    "\n",
    "    for i in range(n_repeat):\n",
    "        for j in range(n_repeat):\n",
    "            y_error += (y_test[:, j] - y_predict[:, i]) ** 2\n",
    "\n",
    "    y_error /= (n_repeat * n_repeat)\n",
    "\n",
    "    y_noise = np.var(y_test, axis=1)\n",
    "    y_bias = (f(X_test) - np.mean(y_predict, axis=1)) ** 2\n",
    "    y_var = np.var(y_predict, axis=1)\n",
    "\n",
    "    print(\"{0}: {1:.4f} (error) = {2:.4f} (bias^2) \"\n",
    "          \" + {3:.4f} (var) + {4:.4f} (noise)\".format(name,\n",
    "                                                      np.mean(y_error),\n",
    "                                                      np.mean(y_bias),\n",
    "                                                      np.mean(y_var),\n",
    "                                                      np.mean(y_noise)))\n",
    "\n",
    "    # Plot figures\n",
    "    if(col==1):\n",
    "        legend = True\n",
    "    else:\n",
    "        legend = False\n",
    "    \n",
    "    for i in range(n_repeat):\n",
    "        if i == 0:\n",
    "            p3 = go.Scatter(x=data_to_plotly(X_test), \n",
    "                            y=y_predict[:, i],\n",
    "                            showlegend=legend,\n",
    "                            mode='lines',\n",
    "                            line=dict(color='red'),\n",
    "                            name=\"<i>y(x)</i>\")\n",
    "            fig.append_trace(p3, row, col)\n",
    "        else:\n",
    "            p3 = go.Scatter(x=data_to_plotly(X_test),\n",
    "                            y=y_predict[:, i], \n",
    "                            showlegend=False,                       \n",
    "                            mode='lines',\n",
    "                            line=dict(color='red', width=0.3))\n",
    "            fig.append_trace(p3, row, col)\n",
    "            \n",
    "    p1 = go.Scatter(x=data_to_plotly(X_test),\n",
    "                    y=f(X_test), \n",
    "                    showlegend=legend,\n",
    "                    mode='lines',\n",
    "                    line=dict(color='blue'),\n",
    "                    name=\"<i>f(x)</i>\")\n",
    "    fig.append_trace(p1, row, col)\n",
    "    \n",
    "    p2 = go.Scatter(x=data_to_plotly(X_train[0]), \n",
    "                    y=y_train[0], \n",
    "                    showlegend=legend,\n",
    "                    mode='lines',\n",
    "                    line=dict(color='blue', dash='dot'),\n",
    "                    name=\"LS ~ <i>y = f(x)+noise</i>\")\n",
    "    fig.append_trace(p2, row, col)\n",
    "\n",
    "    p4 = go.Scatter(x=data_to_plotly(X_test),\n",
    "                    y=np.mean(y_predict, axis=1),\n",
    "                    showlegend=legend,\n",
    "                    mode='lines',\n",
    "                    line=dict(color='cyan'),\n",
    "                    name=\"<i>E<sub>LS</sub>y(x)</i>\")\n",
    "    fig.append_trace(p4, row, col)\n",
    "    \n",
    "    \n",
    "    p5 = go.Scatter(x=data_to_plotly(X_test),\n",
    "                    y=y_error, \n",
    "                    showlegend=legend,\n",
    "                    mode='lines',\n",
    "                    line=dict(color='red'), \n",
    "                    name=\"<i>error(x)</i>\")\n",
    "    fig.append_trace(p5, row+1, col)\n",
    "    \n",
    "    p6 = go.Scatter(x=data_to_plotly(X_test), \n",
    "                    y=y_bias, \n",
    "                    showlegend=legend,\n",
    "                    mode='lines',\n",
    "                    line=dict(color='blue'), \n",
    "                    name=\"<i>bias<sup>2</sup>(x)</i>\")\n",
    "    fig.append_trace(p6, row+1, col)\n",
    "    \n",
    "    p7 = go.Scatter(x=data_to_plotly(X_test),\n",
    "                    y=y_var,\n",
    "                    showlegend=legend,\n",
    "                    mode='lines',\n",
    "                    line=dict(color='green'),  \n",
    "                    name=\"<i>variance(x)</i>\")\n",
    "    fig.append_trace(p7, row+1, col)\n",
    "    \n",
    "    p8 = go.Scatter(x=data_to_plotly(X_test), \n",
    "                    y=y_noise, \n",
    "                    showlegend=legend,\n",
    "                    mode='lines',\n",
    "                    line=dict(color='cyan'),\n",
    "                    name=\"<i>noise(x)</i>\")\n",
    "    fig.append_trace(p8, row+1, col)\n",
    "    \n",
    "    row = 1\n",
    "    col += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The draw time for this plot will be slow for all clients.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diksha/anaconda2/lib/python2.7/site-packages/plotly/plotly/plotly.py:1450: UserWarning:\n",
      "\n",
      "Estimated Draw Time Too Long\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Diksha_Gabha/3059.embed\" height=\"700px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig['layout'].update(height=700, hovermode='closest')\n",
    "\n",
    "for i in map(str, range(1, 5)):\n",
    "    x = 'xaxis' + i\n",
    "    y = 'yaxis' + i\n",
    "    fig['layout'][x].update(showgrid=False, zeroline=False)\n",
    "    fig['layout'][y].update(showgrid=False, zeroline=False)\n",
    "                     \n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T. Hastie, R. Tibshirani and J. Friedman, “Elements of Statistical Learning”, Springer, 2009."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: \n",
    "\n",
    "         Gilles Louppe <g.louppe@gmail.com>\n",
    "         \n",
    "License: \n",
    "\n",
    "         BSD 3 clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href=\"//fonts.googleapis.com/css?family=Open+Sans:600,400,300,200|Inconsolata|Ubuntu+Mono:400,700\" rel=\"stylesheet\" type=\"text/css\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"http://help.plot.ly/documentation/all_static/css/ipython-notebook-custom.css\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/plotly/publisher.git\n",
      "  Cloning https://github.com/plotly/publisher.git to /tmp/pip-t_UxXM-build\n",
      "Installing collected packages: publisher\n",
      "  Found existing installation: publisher 0.10\n",
      "    Uninstalling publisher-0.10:\n",
      "      Successfully uninstalled publisher-0.10\n",
      "  Running setup.py install for publisher ... \u001b[?25l-\b \bdone\n",
      "\u001b[?25hSuccessfully installed publisher-0.10\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML('<link href=\"//fonts.googleapis.com/css?family=Open+Sans:600,400,300,200|Inconsolata|Ubuntu+Mono:400,700\" rel=\"stylesheet\" type=\"text/css\" />'))\n",
    "display(HTML('<link rel=\"stylesheet\" type=\"text/css\" href=\"http://help.plot.ly/documentation/all_static/css/ipython-notebook-custom.css\">'))\n",
    "\n",
    "! pip install git+https://github.com/plotly/publisher.git --upgrade\n",
    "import publisher\n",
    "publisher.publish(\n",
    "    'Single estimator versus bagging bias-variance decomposition.ipynb', 'scikit-learn/plot-bias-variance/', 'Single Estimator Versus Bagging Bias-Variance Decomposition | plotly',\n",
    "    ' ',\n",
    "    title = 'Single Estimator Versus Bagging Bias-Variance Decomposition | plotly',\n",
    "    name = 'Single Estimator Versus Bagging Bias-Variance Decomposition',\n",
    "    has_thumbnail='true', thumbnail='thumbnail/bias-variance.jpg', \n",
    "    language='scikit-learn', page_type='example_index',\n",
    "    display_as='ensemble_methods', order=19,\n",
    "    ipynb= '~Diksha_Gabha/3061')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
