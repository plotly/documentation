{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrate how model complexity influences both prediction accuracy and computational performance.\n",
    "\n",
    "The dataset is the Boston Housing dataset (resp. 20 Newsgroups) for regression (resp. classification).\n",
    "\n",
    "For each class of models we make the model complexity vary through the choice of relevant model parameters and measure the influence on both computational performance (latency) and predictive power (MSE or Hamming Loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.18'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial imports [csr_matrix](http://docs.scipy.org/doc/scipy-0.11.0/reference/generated/scipy.sparse.csr_matrix.getH.html#scipy.sparse.csr_matrix), [shuffle](http://scikit-learn.org/stable/modules/generated/sklearn.utils.shuffle.html#sklearn.utils.shuffle), [mean_squared_error](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error), [NuSVR](http://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVR.html#sklearn.svm.NuSVR), [GradientBoostingRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor), [SGDClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier) and [hamming_loss](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.hamming_loss.html#sklearn.metrics.hamming_loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "print(__doc__)\n",
    "import plotly \n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from mpl_toolkits.axes_grid1.parasite_axes import host_subplot\n",
    "from mpl_toolkits.axisartist.axislines import Axes\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm.classes import NuSVR\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingRegressor\n",
    "from sklearn.linear_model.stochastic_gradient import SGDClassifier\n",
    "from sklearn.metrics import hamming_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "def generate_data(case, sparse=False):\n",
    "    \"\"\"Generate regression/classification data.\"\"\"\n",
    "    bunch = None\n",
    "    if case == 'regression':\n",
    "        bunch = datasets.load_boston()\n",
    "    elif case == 'classification':\n",
    "        bunch = datasets.fetch_20newsgroups_vectorized(subset='all')\n",
    "    X, y = shuffle(bunch.data, bunch.target)\n",
    "    offset = int(X.shape[0] * 0.8)\n",
    "    X_train, y_train = X[:offset], y[:offset]\n",
    "    X_test, y_test = X[offset:], y[offset:]\n",
    "    if sparse:\n",
    "        X_train = csr_matrix(X_train)\n",
    "        X_test = csr_matrix(X_test)\n",
    "    else:\n",
    "        X_train = np.array(X_train)\n",
    "        X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "    y_train = np.array(y_train)\n",
    "    data = {'X_train': X_train, 'X_test': X_test, 'y_train': y_train,\n",
    "            'y_test': y_test}\n",
    "    return data\n",
    "\n",
    "def benchmark_influence(conf):\n",
    "    \"\"\"\n",
    "    Benchmark influence of :changing_param: on both MSE and latency.\n",
    "    \"\"\"\n",
    "    prediction_times = []\n",
    "    prediction_powers = []\n",
    "    complexities = []\n",
    "    for param_value in conf['changing_param_values']:\n",
    "        conf['tuned_params'][conf['changing_param']] = param_value\n",
    "        estimator = conf['estimator'](**conf['tuned_params'])\n",
    "        print(\"Benchmarking %s\" % estimator)\n",
    "        estimator.fit(conf['data']['X_train'], conf['data']['y_train'])\n",
    "        conf['postfit_hook'](estimator)\n",
    "        complexity = conf['complexity_computer'](estimator)\n",
    "        complexities.append(complexity)\n",
    "        start_time = time.time()\n",
    "        for _ in range(conf['n_samples']):\n",
    "            y_pred = estimator.predict(conf['data']['X_test'])\n",
    "        elapsed_time = (time.time() - start_time) / float(conf['n_samples'])\n",
    "        prediction_times.append(elapsed_time)\n",
    "        pred_score = conf['prediction_performance_computer'](\n",
    "            conf['data']['y_test'], y_pred)\n",
    "        prediction_powers.append(pred_score)\n",
    "        print(\"Complexity: %d | %s: %.4f | Pred. Time: %fs\\n\" % (\n",
    "            complexity, conf['prediction_performance_label'], pred_score,\n",
    "            elapsed_time))\n",
    "    return prediction_powers, prediction_times, complexities\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot influence of model complexity on both accuracy and latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_influence(conf, mse_values, prediction_times, complexities):\n",
    "    p1 = go.Scatter(x=complexities, y = mse_values,\n",
    "                    name=\"prediction error\", mode=\"lines\",\n",
    "                    line=dict(color=\"blue\"))\n",
    "    p2 = go.Scatter(x=complexities, y= prediction_times, \n",
    "                    name=\"latency\",mode=\"lines\", yaxis='y2',\n",
    "                    line=dict(color=\"red\") )\n",
    "    layout = go.Layout(\n",
    "        title='Influence of Model Complexity - %s' % conf['estimator'].__name__,\n",
    "        xaxis = dict(\n",
    "            title='Model Complexity (%s)' % conf['complexity_label'],\n",
    "            showgrid=False,),\n",
    "        yaxis=dict(\n",
    "            title=conf['prediction_performance_label'],\n",
    "            titlefont=dict(\n",
    "                    color='blue'),\n",
    "            showgrid=False),\n",
    "        yaxis2=dict(\n",
    "            title='Time (s)',\n",
    "            showgrid=False,\n",
    "            titlefont=dict(\n",
    "                    color='red'),\n",
    "            overlaying='y',\n",
    "            side='right'\n",
    "    ))\n",
    "   \n",
    "    fig = go.Figure(data=[p1,p2], layout=layout)\n",
    "    return fig\n",
    "\n",
    "def _count_nonzero_coefficients(estimator):\n",
    "    a = estimator.coef_.toarray()\n",
    "    return np.count_nonzero(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.25,\n",
      "       learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "Complexity: 4454 | Hamming Loss (Misclassification Ratio): 0.2501 | Pred. Time: 0.024822s\n",
      "\n",
      "Benchmarking SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.5, learning_rate='optimal',\n",
      "       loss='modified_huber', n_iter=5, n_jobs=1, penalty='elasticnet',\n",
      "       power_t=0.5, random_state=None, shuffle=True, verbose=0,\n",
      "       warm_start=False)\n",
      "Complexity: 1624 | Hamming Loss (Misclassification Ratio): 0.2923 | Pred. Time: 0.020898s\n",
      "\n",
      "Benchmarking SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.75,\n",
      "       learning_rate='optimal', loss='modified_huber', n_iter=5, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "Complexity: 873 | Hamming Loss (Misclassification Ratio): 0.3191 | Pred. Time: 0.015077s\n",
      "\n",
      "Benchmarking SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.9, learning_rate='optimal',\n",
      "       loss='modified_huber', n_iter=5, n_jobs=1, penalty='elasticnet',\n",
      "       power_t=0.5, random_state=None, shuffle=True, verbose=0,\n",
      "       warm_start=False)\n",
      "Complexity: 655 | Hamming Loss (Misclassification Ratio): 0.3252 | Pred. Time: 0.013285s\n",
      "\n",
      "Benchmarking NuSVR(C=1000.0, cache_size=200, coef0=0.0, degree=3, gamma=3.0517578125e-05,\n",
      "   kernel='rbf', max_iter=-1, nu=0.1, shrinking=True, tol=0.001,\n",
      "   verbose=False)\n",
      "Complexity: 69 | MSE: 31.8133 | Pred. Time: 0.000330s\n",
      "\n",
      "Benchmarking NuSVR(C=1000.0, cache_size=200, coef0=0.0, degree=3, gamma=3.0517578125e-05,\n",
      "   kernel='rbf', max_iter=-1, nu=0.25, shrinking=True, tol=0.001,\n",
      "   verbose=False)\n",
      "Complexity: 136 | MSE: 25.6140 | Pred. Time: 0.000664s\n",
      "\n",
      "Benchmarking NuSVR(C=1000.0, cache_size=200, coef0=0.0, degree=3, gamma=3.0517578125e-05,\n",
      "   kernel='rbf', max_iter=-1, nu=0.5, shrinking=True, tol=0.001,\n",
      "   verbose=False)\n",
      "Complexity: 243 | MSE: 22.3315 | Pred. Time: 0.001050s\n",
      "\n",
      "Benchmarking NuSVR(C=1000.0, cache_size=200, coef0=0.0, degree=3, gamma=3.0517578125e-05,\n",
      "   kernel='rbf', max_iter=-1, nu=0.75, shrinking=True, tol=0.001,\n",
      "   verbose=False)\n",
      "Complexity: 350 | MSE: 21.3679 | Pred. Time: 0.002377s\n",
      "\n",
      "Benchmarking NuSVR(C=1000.0, cache_size=200, coef0=0.0, degree=3, gamma=3.0517578125e-05,\n",
      "   kernel='rbf', max_iter=-1, nu=0.9, shrinking=True, tol=0.001,\n",
      "   verbose=False)\n",
      "Complexity: 404 | MSE: 21.0915 | Pred. Time: 0.002316s\n",
      "\n",
      "Benchmarking GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
      "             max_leaf_nodes=None, min_impurity_split=1e-07,\n",
      "             min_samples_leaf=1, min_samples_split=2,\n",
      "             min_weight_fraction_leaf=0.0, n_estimators=10, presort='auto',\n",
      "             random_state=None, subsample=1.0, verbose=0, warm_start=False)\n",
      "Complexity: 10 | MSE: 28.9793 | Pred. Time: 0.000074s\n",
      "\n",
      "Benchmarking GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
      "             max_leaf_nodes=None, min_impurity_split=1e-07,\n",
      "             min_samples_leaf=1, min_samples_split=2,\n",
      "             min_weight_fraction_leaf=0.0, n_estimators=50, presort='auto',\n",
      "             random_state=None, subsample=1.0, verbose=0, warm_start=False)\n",
      "Complexity: 50 | MSE: 8.3398 | Pred. Time: 0.000210s\n",
      "\n",
      "Benchmarking GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
      "             max_leaf_nodes=None, min_impurity_split=1e-07,\n",
      "             min_samples_leaf=1, min_samples_split=2,\n",
      "             min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "             presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "             warm_start=False)\n",
      "Complexity: 100 | MSE: 7.0096 | Pred. Time: 0.000301s\n",
      "\n",
      "Benchmarking GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
      "             max_leaf_nodes=None, min_impurity_split=1e-07,\n",
      "             min_samples_leaf=1, min_samples_split=2,\n",
      "             min_weight_fraction_leaf=0.0, n_estimators=200,\n",
      "             presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "             warm_start=False)\n",
      "Complexity: 200 | MSE: 6.1836 | Pred. Time: 0.000559s\n",
      "\n",
      "Benchmarking GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
      "             max_leaf_nodes=None, min_impurity_split=1e-07,\n",
      "             min_samples_leaf=1, min_samples_split=2,\n",
      "             min_weight_fraction_leaf=0.0, n_estimators=500,\n",
      "             presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
      "             warm_start=False)\n",
      "Complexity: 500 | MSE: 6.3426 | Pred. Time: 0.000905s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regression_data = generate_data('regression')\n",
    "classification_data = generate_data('classification', sparse=True)\n",
    "configurations = [\n",
    "    {'estimator': SGDClassifier,\n",
    "     'tuned_params': {'penalty': 'elasticnet', 'alpha': 0.001, 'loss':\n",
    "                      'modified_huber', 'fit_intercept': True},\n",
    "     'changing_param': 'l1_ratio',\n",
    "     'changing_param_values': [0.25, 0.5, 0.75, 0.9],\n",
    "     'complexity_label': 'non_zero coefficients',\n",
    "     'complexity_computer': _count_nonzero_coefficients,\n",
    "     'prediction_performance_computer': hamming_loss,\n",
    "     'prediction_performance_label': 'Hamming Loss (Misclassification Ratio)',\n",
    "     'postfit_hook': lambda x: x.sparsify(),\n",
    "     'data': classification_data,\n",
    "     'n_samples': 30},\n",
    "    {'estimator': NuSVR,\n",
    "     'tuned_params': {'C': 1e3, 'gamma': 2 ** -15},\n",
    "     'changing_param': 'nu',\n",
    "     'changing_param_values': [0.1, 0.25, 0.5, 0.75, 0.9],\n",
    "     'complexity_label': 'n_support_vectors',\n",
    "     'complexity_computer': lambda x: len(x.support_vectors_),\n",
    "     'data': regression_data,\n",
    "     'postfit_hook': lambda x: x,\n",
    "     'prediction_performance_computer': mean_squared_error,\n",
    "     'prediction_performance_label': 'MSE',\n",
    "     'n_samples': 30},\n",
    "    {'estimator': GradientBoostingRegressor,\n",
    "     'tuned_params': {'loss': 'ls'},\n",
    "     'changing_param': 'n_estimators',\n",
    "     'changing_param_values': [10, 50, 100, 200, 500],\n",
    "     'complexity_label': 'n_trees',\n",
    "     'complexity_computer': lambda x: x.n_estimators,\n",
    "     'data': regression_data,\n",
    "     'postfit_hook': lambda x: x,\n",
    "     'prediction_performance_computer': mean_squared_error,\n",
    "     'prediction_performance_label': 'MSE',\n",
    "     'n_samples': 30},\n",
    "]\n",
    "\n",
    "model_compexity_influence_plot = []\n",
    "for conf in configurations:\n",
    "    prediction_performances, prediction_times, complexities = \\\n",
    "        benchmark_influence(conf)\n",
    "    trace = plot_influence(conf, prediction_performances, prediction_times,\n",
    "                   complexities)\n",
    "    model_compexity_influence_plot.append(trace)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Model Complexity Influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Diksha_Gabha/2635.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " py.iplot(model_compexity_influence_plot[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Diksha_Gabha/2637.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " py.iplot(model_compexity_influence_plot[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Diksha_Gabha/2639.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " py.iplot(model_compexity_influence_plot[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: \n",
    "    \n",
    "        Eustache Diemert <eustache@diemert.fr>\n",
    "License: \n",
    "    \n",
    "        BSD 3 clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href=\"//fonts.googleapis.com/css?family=Open+Sans:600,400,300,200|Inconsolata|Ubuntu+Mono:400,700\" rel=\"stylesheet\" type=\"text/css\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"http://help.plot.ly/documentation/all_static/css/ipython-notebook-custom.css\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/plotly/publisher.git\n",
      "  Cloning https://github.com/plotly/publisher.git to /tmp/pip-LJLNCR-build\n",
      "Installing collected packages: publisher\n",
      "  Running setup.py install for publisher ... \u001b[?25l-\b \berror\n",
      "    Complete output from command /usr/bin/python -u -c \"import setuptools, tokenize;__file__='/tmp/pip-LJLNCR-build/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))\" install --record /tmp/pip-Qsh7Lb-record/install-record.txt --single-version-externally-managed --compile:\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build/lib.linux-x86_64-2.7\n",
      "    creating build/lib.linux-x86_64-2.7/publisher\n",
      "    copying publisher/publisher.py -> build/lib.linux-x86_64-2.7/publisher\n",
      "    copying publisher/__init__.py -> build/lib.linux-x86_64-2.7/publisher\n",
      "    running install_lib\n",
      "    creating /usr/local/lib/python2.7/dist-packages/publisher\n",
      "    error: could not create '/usr/local/lib/python2.7/dist-packages/publisher': Permission denied\n",
      "    \n",
      "    ----------------------------------------\n",
      "\u001b[31mCommand \"/usr/bin/python -u -c \"import setuptools, tokenize;__file__='/tmp/pip-LJLNCR-build/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))\" install --record /tmp/pip-Qsh7Lb-record/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in /tmp/pip-LJLNCR-build/\u001b[0m\n",
      "\u001b[33mYou are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/diksha/anaconda2/lib/python2.7/site-packages/IPython/nbconvert.py:13: ShimWarning: The `IPython.nbconvert` package has been deprecated. You should import from nbconvert instead.\n",
      "  \"You should import from nbconvert instead.\", ShimWarning)\n",
      "/home/diksha/anaconda2/lib/python2.7/site-packages/publisher/publisher.py:53: UserWarning: Did you \"Save\" this notebook before running this command? Remember to save, always save.\n",
      "  warnings.warn('Did you \"Save\" this notebook before running this command? '\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML('<link href=\"//fonts.googleapis.com/css?family=Open+Sans:600,400,300,200|Inconsolata|Ubuntu+Mono:400,700\" rel=\"stylesheet\" type=\"text/css\" />'))\n",
    "display(HTML('<link rel=\"stylesheet\" type=\"text/css\" href=\"http://help.plot.ly/documentation/all_static/css/ipython-notebook-custom.css\">'))\n",
    "\n",
    "! pip install git+https://github.com/plotly/publisher.git --upgrade\n",
    "import publisher\n",
    "publisher.publish(\n",
    "    'model-complexity.ipynb', 'scikit-learn/plot-model-complexity-influence/', 'Model Complexity Influence | plotly',\n",
    "    ' ',\n",
    "    title = 'Model Complexity Influence | plotly',\n",
    "    name = 'Model Complexity Influence',\n",
    "    has_thumbnail='true', thumbnail='thumbnail/model-complexity.jpg', \n",
    "    language='scikit-learn', page_type='example_index',\n",
    "    display_as='real_dataset', order=4, ipynb='~Diksha_Gabha/2669')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
