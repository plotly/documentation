{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example visualizes some training loss curves for different stochastic learning strategies, including SGD and Adam. Because of time-constraints, we use several small datasets, for which L-BFGS might be more suitable. The general trend shown in these examples seems to carry over to larger datasets, however.\n",
    "Note that those results can be highly dependent on the value of learning_rate_init."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New to Plotly?\n",
    "Plotly's Python library is free and open source! [Get started](https://plot.ly/python/getting-started/) by downloading the client and [reading the primer](https://plot.ly/python/getting-started/).\n",
    "<br>You can set up Plotly to work in [online](https://plot.ly/python/getting-started/#initialization-for-online-plotting) or [offline](https://plot.ly/python/getting-started/#initialization-for-offline-plotting) mode, or in [jupyter notebooks](https://plot.ly/python/getting-started/#start-plotting-online).\n",
    "<br>We also have a quick-reference [cheatsheet](https://images.plot.ly/plotly-documentation/images/python_cheat_sheet.pdf) (new!) to help you get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.18.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial imports [MLPClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier) and [MinMaxScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# different learning rate schedules and momentum parameters\n",
    "params = [{'solver': 'sgd', 'learning_rate': 'constant', 'momentum': 0,\n",
    "           'learning_rate_init': 0.2},\n",
    "          {'solver': 'sgd', 'learning_rate': 'constant', 'momentum': .9,\n",
    "           'nesterovs_momentum': False, 'learning_rate_init': 0.2},\n",
    "          {'solver': 'sgd', 'learning_rate': 'constant', 'momentum': .9,\n",
    "           'nesterovs_momentum': True, 'learning_rate_init': 0.2},\n",
    "          {'solver': 'sgd', 'learning_rate': 'invscaling', 'momentum': 0,\n",
    "           'learning_rate_init': 0.2},\n",
    "          {'solver': 'sgd', 'learning_rate': 'invscaling', 'momentum': .9,\n",
    "           'nesterovs_momentum': True, 'learning_rate_init': 0.2},\n",
    "          {'solver': 'sgd', 'learning_rate': 'invscaling', 'momentum': .9,\n",
    "           'nesterovs_momentum': False, 'learning_rate_init': 0.2},\n",
    "          {'solver': 'adam', 'learning_rate_init': 0.01}]\n",
    " \n",
    "labels = [\"constant learning-rate\", \"constant with momentum\",\n",
    "          \"constant with Nesterov's momentum\",\n",
    "          \"inv-scaling learning-rate\", \"inv-scaling with momentum\",\n",
    "          \"inv-scaling with Nesterov's momentum\", \"adam\"]\n",
    "\n",
    "plot_args = [{'c': 'red', 'linestyle': '-'},\n",
    "             {'c': 'green', 'linestyle': '-'},\n",
    "             {'c': 'blue', 'linestyle': '-'},\n",
    "             {'c': 'red', 'linestyle': '--'},\n",
    "             {'c': 'green', 'linestyle': '--'},\n",
    "             {'c': 'blue', 'linestyle': '--'},\n",
    "             {'c': 'black', 'linestyle': '-'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_on_dataset(X, y, name, leg):\n",
    "    # for each dataset, plot learning for each learning strategy\n",
    "    print(\"\\nlearning on dataset %s\" % name)\n",
    "    \n",
    "    X = MinMaxScaler().fit_transform(X)\n",
    "    mlps = []\n",
    "    if name == \"digits\":\n",
    "        # digits is larger but converges fairly quickly\n",
    "        max_iter = 15\n",
    "    else:\n",
    "        max_iter = 400\n",
    "\n",
    "    for label, param in zip(labels, params):\n",
    "        print(\"training: %s\" % label)\n",
    "        mlp = MLPClassifier(verbose=0, random_state=0,\n",
    "                            max_iter=max_iter, **param)\n",
    "        mlp.fit(X, y)\n",
    "        mlps.append(mlp)\n",
    "        print(\"Training set score: %f\" % mlp.score(X, y))\n",
    "        print(\"Training set loss: %f\" % mlp.loss_)\n",
    "        \n",
    "    data = []\n",
    "    \n",
    "    for mlp, label, args in zip(mlps, labels, plot_args):\n",
    "            trace = go.Scatter(y=mlp.loss_curve_, name=label,\n",
    "                               mode='lines', showlegend=leg,\n",
    "                               line=dict(width=1))\n",
    "            data.append(trace)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "learning on dataset iris\n",
      "training: constant learning-rate\n",
      "Training set score: 0.980000\n",
      "Training set loss: 0.096922\n",
      "training: constant with momentum\n",
      "Training set score: 0.980000\n",
      "Training set loss: 0.050260\n",
      "training: constant with Nesterov's momentum\n",
      "Training set score: 0.980000\n",
      "Training set loss: 0.050277\n",
      "training: inv-scaling learning-rate\n",
      "Training set score: 0.360000\n",
      "Training set loss: 0.979983\n",
      "training: inv-scaling with momentum\n",
      "Training set score: 0.860000\n",
      "Training set loss: 0.504017\n",
      "training: inv-scaling with Nesterov's momentum\n",
      "Training set score: 0.860000\n",
      "Training set loss: 0.504760\n",
      "training: adam\n",
      "Training set score: 0.980000\n",
      "Training set loss: 0.046248\n",
      "\n",
      "learning on dataset digits\n",
      "training: constant learning-rate\n",
      "Training set score: 0.956038\n",
      "Training set loss: 0.243802\n",
      "training: constant with momentum\n",
      "Training set score: 0.992766\n",
      "Training set loss: 0.041297\n",
      "training: constant with Nesterov's momentum\n",
      "Training set score: 0.993879\n",
      "Training set loss: 0.042898\n",
      "training: inv-scaling learning-rate\n",
      "Training set score: 0.638843\n",
      "Training set loss: 1.855465\n",
      "training: inv-scaling with momentum\n",
      "Training set score: 0.912632\n",
      "Training set loss: 0.290584\n",
      "training: inv-scaling with Nesterov's momentum\n",
      "Training set score: 0.909293\n",
      "Training set loss: 0.318387\n",
      "training: adam\n",
      "Training set score: 0.991653\n",
      "Training set loss: 0.045934\n",
      "\n",
      "learning on dataset circles\n",
      "training: constant learning-rate\n",
      "Training set score: 0.830000\n",
      "Training set loss: 0.681498\n",
      "training: constant with momentum\n",
      "Training set score: 0.940000\n",
      "Training set loss: 0.163712\n",
      "training: constant with Nesterov's momentum\n",
      "Training set score: 0.940000\n",
      "Training set loss: 0.163012\n",
      "training: inv-scaling learning-rate\n",
      "Training set score: 0.500000\n",
      "Training set loss: 0.692855\n",
      "training: inv-scaling with momentum\n",
      "Training set score: 0.510000\n",
      "Training set loss: 0.688376\n",
      "training: inv-scaling with Nesterov's momentum\n",
      "Training set score: 0.500000\n",
      "Training set loss: 0.688593\n",
      "training: adam\n",
      "Training set score: 0.930000\n",
      "Training set loss: 0.159988\n",
      "\n",
      "learning on dataset moons\n",
      "training: constant learning-rate\n",
      "Training set score: 0.850000\n",
      "Training set loss: 0.342245\n",
      "training: constant with momentum\n",
      "Training set score: 0.850000\n",
      "Training set loss: 0.345580\n",
      "training: constant with Nesterov's momentum\n",
      "Training set score: 0.850000\n",
      "Training set loss: 0.336284\n",
      "training: inv-scaling learning-rate\n",
      "Training set score: 0.500000\n",
      "Training set loss: 0.689729\n",
      "training: inv-scaling with momentum\n",
      "Training set score: 0.830000\n",
      "Training set loss: 0.512595\n",
      "training: inv-scaling with Nesterov's momentum\n",
      "Training set score: 0.830000\n",
      "Training set loss: 0.513034\n",
      "training: adam\n",
      "Training set score: 0.850000\n",
      "Training set loss: 0.334243\n"
     ]
    }
   ],
   "source": [
    "# load / generate some toy datasets\n",
    "iris = datasets.load_iris()\n",
    "digits = datasets.load_digits()\n",
    "data_sets = [(iris.data, iris.target),\n",
    "             (digits.data, digits.target),\n",
    "             datasets.make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "             datasets.make_moons(noise=0.3, random_state=0)]\n",
    "\n",
    "names = ['iris', 'digits', 'circles', 'moons']\n",
    "fig = tools.make_subplots(rows=2, cols=2, print_grid=False,\n",
    "                          subplot_titles=tuple(names))\n",
    "\n",
    "for i, data, name in zip(np.linspace(0, 3, 4), \n",
    "                         data_sets, names):\n",
    "    if(i==0):\n",
    "        leg=True\n",
    "    else:\n",
    "        leg=False\n",
    "    trace = plot_on_dataset(*data, name=name, leg=leg)\n",
    "    for j in range(0, len(trace)):\n",
    "         fig.append_trace(trace[j], int(i/2+1), int(i%2+1))\n",
    "            \n",
    "fig['layout'].update(height=700, hovermode='closest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~Diksha_Gabha/3494.embed\" height=\"700px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href=\"//fonts.googleapis.com/css?family=Open+Sans:600,400,300,200|Inconsolata|Ubuntu+Mono:400,700\" rel=\"stylesheet\" type=\"text/css\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"http://help.plot.ly/documentation/all_static/css/ipython-notebook-custom.css\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/plotly/publisher.git\n",
      "  Cloning https://github.com/plotly/publisher.git to /tmp/pip-7gC2YK-build\n",
      "Installing collected packages: publisher\n",
      "  Found existing installation: publisher 0.10\n",
      "    Uninstalling publisher-0.10:\n",
      "      Successfully uninstalled publisher-0.10\n",
      "  Running setup.py install for publisher ... \u001b[?25l-\b \bdone\n",
      "\u001b[?25hSuccessfully installed publisher-0.10\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML('<link href=\"//fonts.googleapis.com/css?family=Open+Sans:600,400,300,200|Inconsolata|Ubuntu+Mono:400,700\" rel=\"stylesheet\" type=\"text/css\" />'))\n",
    "display(HTML('<link rel=\"stylesheet\" type=\"text/css\" href=\"http://help.plot.ly/documentation/all_static/css/ipython-notebook-custom.css\">'))\n",
    "\n",
    "! pip install git+https://github.com/plotly/publisher.git --upgrade\n",
    "import publisher\n",
    "publisher.publish(\n",
    "    'Compare Stochastic Learning Strategies for MLPClassifier.ipynb', 'scikit-learn/plot-mlp-training-curves/', 'Compare Stochastic Learning Strategies for MLPClassifier | plotly',\n",
    "    ' ',\n",
    "    title = 'Compare Stochastic Learning Strategies for MLPClassifier | plotly',\n",
    "    name = 'Compare Stochastic Learning Strategies for MLPClassifier',\n",
    "    has_thumbnail='true', thumbnail='thumbnail/mlp-training.jpg', \n",
    "    language='scikit-learn', page_type='example_index',\n",
    "    display_as='neural_networks', order=3,\n",
    "    ipynb= '~Diksha_Gabha/3496')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
